import streamlit as st
import os
import fitz
import docx
import pandas as pd
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

# Google Drive Authentication
def load_drive():
    gauth = GoogleAuth()
    gauth.settings.update({
        "client_config": {
            "client_id": st.secrets["google_oauth"]["client_id"],
            "client_secret": st.secrets["google_oauth"]["client_secret"],
            "auth_uri": "https://accounts.google.com/o/oauth2/auth",
            "token_uri": "https://accounts.google.com/o/oauth2/token",
            "redirect_uris": ["urn:ietf:wg:oauth:2.0:oob", "http://localhost"],
        }
    })
    gauth.LocalWebserverAuth()
    drive = GoogleDrive(gauth)
    return drive
def download_folder(drive, folder_id, local_path):
    if not os.path.exists(local_path):
        os.makedirs(local_path)

    file_list = drive.ListFile({'q': f"'{folder_id}' in parents and trashed=false"}).GetList()

    for file in file_list:
        fname = os.path.join(local_path, file['title'])
        file.GetContentFile(fname)
        print("Downloaded:", fname)

# client = OpenAI(api_key=)   
api_key = st.secrets.get("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

# File Extraction :
def extract_pdf(path, filename):
    doc = fitz.open(path)
    pages = []

    for i, page in enumerate(doc):
        text = page.get_text("text")
        if text.strip():
            pages.append({
                "id": f"{filename}__page_{i+1}",
                "source": filename,
                "type": "pdf",
                "text": text.strip()
            })
    return pages

def extract_word(path, filename):
    try:
        doc_file = docx.Document(path)
    except:
        return []

    output = []

    # Paragraphs
    paragraphs = [p.text.strip() for p in doc_file.paragraphs if p.text.strip()]
    if paragraphs:
        output.append({
            "id": f"{filename}__paragraphs",
            "source": filename,
            "type": "word",
            "text": "\n".join(paragraphs)
        })

    # Tables
    table_index = 0
    for table in doc_file.tables:
        table_index += 1
        rows_text = []
        for row in table.rows:
            cells = [cell.text.strip() for cell in row.cells]
            rows_text.append(" | ".join(cells))
        table_text = "\n".join(rows_text).strip()

        if table_text:
            output.append({
                "id": f"{filename}__table_{table_index}",
                "source": filename,
                "type": "word_table",
                "text": table_text
            })

    return output

def extract_excel(path, filename):
    try:
        sheets = pd.read_excel(path, sheet_name=None)
    except:
        return []

    output = []

    for sheet, df in sheets.items():
        if df.empty:
            continue

        headers = list(df.columns)

        for idx, row in df.iterrows():
            fields = []
            for h, v in zip(headers, row.tolist()):
                v = str(v).strip()
                if v.lower() == "nan" or v == "":
                    continue
                fields.append(f"{h}: {v}")

            if fields:
                row_txt = "\n".join(fields)
                output.append({
                    "id": f"{filename}__sheet_{sheet}__row_{idx}",
                    "source": filename,
                    "type": "excel",
                    "text": row_txt
                })

    return output

def process_dir(folder):
    records = []

    for fname in os.listdir(folder):
        path = os.path.join(folder, fname)
        ext = fname.lower().split(".")[-1]

        if ext == "pdf":
            recs = extract_pdf(path, fname)
        elif ext == "docx":
            recs = extract_word(path, fname)
        elif ext in ["xlsx", "xls"]:
            recs = extract_excel(path, fname)
        else:
            continue

        records.extend(recs)

    return records

# Embeddings & Knoweldge Database
def build_faiss_index(texts):
    model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2") # General Model
    embs = model.encode(texts, show_progress_bar=True)
    embs = np.asarray(embs).astype("float32")

    index = faiss.IndexFlatL2(embs.shape[1])
    index.add(embs)

    return index, model, embs

# RAG Search
def search(query, model, index, ids, texts, k=10):
    q_emb = model.encode([query], convert_to_numpy=True).astype("float32")
    D, I = index.search(q_emb, k)

    results = []
    for rank, idx in enumerate(I[0]):
        results.append({
            "rank": rank + 1,
            "score": float(D[0][rank]),
            "id": ids[idx],
            "text": texts[idx]
        })
    return results

# LLM Prompt
def build_context(results):
    ctx = ""
    for r in results:
        ctx += f"\n\n--- Ù…Ù† Ø§Ù„Ù…Ù„Ù: {r['id']} ---\n{r['text']}"
    return ctx


def llm_answer(query, results):
    context = build_context(results)

    prompt = f"""
Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ§Ù„ÙŠ:

Ø§Ù„Ø³Ø¤Ø§Ù„:
{query}

Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¬Ø§Ù…Ø¹ÙŠ Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ù‹Ø§ ÙˆÙ„Ø§ ØªØ°ÙƒØ± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª."},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content

# STREAMLIT UI
st.title("SEU Chatbot")

# FAQ_DIR = st.text_input("ğŸ“ Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ FAQ", r"D:\Azzam\Personal_Projects\SEU\filtered_data\FAQ")
# DOCS_DIR = st.text_input("ğŸ“ Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ Docs", r"D:\Azzam\Personal_Projects\SEU\filtered_data\Docs")
FAQ_DIR = st.text_input("ğŸ“ Ù…Ø¬Ù„Ø¯ FAQ", "FAQ")
DOCS_DIR = st.text_input("ğŸ“ Ù…Ø¬Ù„Ø¯ Docs", "Docs")

if st.button("Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"):
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª..."):
        with st.spinner("Downloading files from Google Drive..."):
            drive = load_drive()
            download_folder(drive, FAQ_DRIVE_ID, "FAQ")
            download_folder(drive, DOCS_DRIVE_ID, "Docs")
    
        st.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Google Drive!")
        records = process_dir(FAQ_DIR)
        records += process_dir(DOCS_DIR)

        st.success(f"âœ” ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(records)} Ø¬Ø²Ø¡ Ù†ØµÙŠ!")

        texts = [rec["text"] for rec in records]
        ids = [rec["id"] for rec in records]

        st.info("Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Embeddings + FAISS...")
        index, model, embs = build_faiss_index(texts)

        st.session_state["records"] = records
        st.session_state["texts"] = texts
        st.session_state["ids"] = ids
        st.session_state["index"] = index
        st.session_state["model"] = model

# CHAT SECTION
st.subheader("Ø§Ø³Ø£Ù„...")

query = st.text_input("Ø§ÙƒØªØ¨ Ù‡Ù†Ø§â€¦")

if st.button("Ø¥Ø±Ø³Ø§Ù„"):
    if "index" not in st.session_state:
        st.error("ÙŠØ¬Ø¨ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø£ÙˆÙ„Ø§Ù‹!")
    else:
        model = st.session_state["model"]
        index = st.session_state["index"]
        ids = st.session_state["ids"]
        texts = st.session_state["texts"]

        results = search(query, model, index, ids, texts, k=8)
        answer = llm_answer(query, results)

        st.markdown("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©")
        st.write(answer)

        st.markdown("---")
        st.markdown("Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©")
        for r in results:
            st.write(f"**{r['id']}** â€” Score: {r['score']}")
            st.write(r["text"])
            st.write("---")


